{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install jupyter notebook\n",
    "#conda install -c conda-forge opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea131a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5167b4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load image: 1.PNG\n",
      "Failed to load image: 2.PNG\n",
      "Failed to load image: 3.PNG\n",
      "Failed to load image: 4.PNG\n",
      "Failed to load image: 5.PNG\n",
      "Failed to load image: 캡처1.PNG\n",
      "Failed to load image: 테슬라.PNG\n"
     ]
    }
   ],
   "source": [
    "# Define the input directory containing the image files\n",
    "input_dir = \"C:/Users/tiock/Desktop/창균/\"\n",
    "\n",
    "# Define the output directory for the resized images\n",
    "output_dir = \"C:/Users/tiock/Desktop/창균/test/\"\n",
    "\n",
    "output_size = (224, 224)\n",
    "\n",
    "# Loop over all the image files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Get the file extension\n",
    "    file_ext = os.path.splitext(filename)[1]\n",
    "    \n",
    "    # Skip files that are not png or jpg\n",
    "    if file_ext.lower() not in [\".png\", \".jpg\"]:\n",
    "        continue\n",
    "        \n",
    "    # Load the image from file\n",
    "    image = cv2.imread(os.path.join(input_dir, filename))\n",
    "    \n",
    "    # Make sure the image is not empty\n",
    "    if image is None:\n",
    "        print(\"Failed to load image:\", filename)\n",
    "        continue\n",
    "    \n",
    "    # Resize the image to the desired output size\n",
    "    image = cv2.resize(image, output_size)\n",
    "    \n",
    "    # Save the resized image to file\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features):\n",
    "        self.weights = np.zeros(num_features + 1)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)\n",
    "        return np.where(np.dot(self.weights, x) >= 0.0, 1, -1)\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            for i, x in enumerate(X):\n",
    "                x = np.insert(x, 0, 1)\n",
    "                prediction = self.predict(x)\n",
    "                error = y[i] - prediction\n",
    "                self.weights += learning_rate * error * x\n",
    "\n",
    "def OR(x1, x2):\n",
    "    X = np.array([[x1, x2]])\n",
    "    y = np.array([1 if x1 == 1 or x2 == 1 else -1])\n",
    "    model = Perceptron(num_features=2)\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X[0])[0]\n",
    "\n",
    "def AND(x1, x2):\n",
    "    X = np.array([[x1, x2]])\n",
    "    y = np.array([1 if x1 == 1 and x2 == 1 else -1])\n",
    "    model = Perceptron(num_features=2)\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X[0])[0]\n",
    "\n",
    "def XOR(x1, x2):\n",
    "    X = np.array([[x1, x2]])\n",
    "    y = np.array([1 if x1 != x2 else -1])\n",
    "    model = Perceptron(num_features=2)\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b78748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e803a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어사전\n",
    "os.getcwd()\n",
    "path=\"C:/Users/tiock/Desktop/수업자료/인공지능/text/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0c0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive_words.txt', 'r',encoding='utf-8') as f:\n",
    "    positive_words = f.read().splitlines()\n",
    "\n",
    "with open('negative_words.txt', 'r',encoding='utf-8') as f:\n",
    "    negative_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6586bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dict = {}\n",
    "for word in positive_words:\n",
    "    positive_dict[word] = 0\n",
    "\n",
    "negative_dict = {}\n",
    "for word in negative_words:\n",
    "    negative_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758a6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {}\n",
    "sentiment_dict.update(positive_dict)\n",
    "sentiment_dict.update(negative_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcc115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d609c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keys_list = list(negative_dict.keys())\n",
    "n_values_list = list(negative_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c72698",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_keys_list = list(positive_dict.keys())\n",
    "p_values_list = list(positive_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd68e66",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 데이터셋 준비 및 전처리\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 데이터셋 준비 및 전처리\n",
    "positive_sentences = p_keys_list  # 긍정적인 문장들을 담은 리스트\n",
    "negative_sentences = n_keys_list  # 부정적인 문장들을 담은 리스트\n",
    "\n",
    "sentences = positive_sentences + negative_sentences\n",
    "labels = [1] * len(positive_sentences) + [0] * len(negative_sentences)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "X = vectorizer.transform(sentences)\n",
    "\n",
    "# 모델 학습\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, labels)\n",
    "\n",
    "# 모델 적용\n",
    "new_sentence = \"이 영화의 감독과 줄거리는 최고였다. 하지만 영화 내용은 지루하고 싫다.\"\n",
    "new_sentence_vectorized = vectorizer.transform([new_sentence])\n",
    "prediction = clf.predict(new_sentence_vectorized)\n",
    "\n",
    "if prediction == 1:\n",
    "    print(\"긍정적인 문장입니다.\")\n",
    "else:\n",
    "    print(\"부정적인 문장입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5e08d1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "07c36cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, vectorizer, clf):\n",
    "    vectorized_text = vectorizer.transform([text])\n",
    "    prediction = clf.predict(vectorized_text)[0]\n",
    "    if prediction == 1:\n",
    "        return \"긍정적인 문장입니다.\"\n",
    "    else:\n",
    "        return \"부정적인 문장입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3fe1fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    # 입력된 텍스트를 단어 단위로 분리하고, 각 단어가 긍정 단어인지 부정 단어인지 확인합니다.\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            positive_count += 1\n",
    "        elif word in negative_words:\n",
    "            negative_count += 1\n",
    "\n",
    "    # 긍정 단어의 개수와 부정 단어의 개수를 비교하여, 해당 텍스트의 감성을 판단합니다.\n",
    "    if positive_count > negative_count:\n",
    "        return \"긍정적인 문장입니다.\"\n",
    "    elif positive_count < negative_count:\n",
    "        return \"부정적인 문장입니다.\"\n",
    "    else:\n",
    "        return \"중립적인 문장입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1581bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석할 문자열을 입력하세요: \n",
      "긍정적인 문장입니다.\n"
     ]
    }
   ],
   "source": [
    "new_text = input(\"분석할 문자열을 입력하세요: \")\n",
    "result = predict_sentiment(new_text, vectorizer, clf)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "17be3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#업그레이드 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "093c9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_sentiment_analysis_model(positive_file, negative_file):\n",
    "    # 긍정 단어와 부정 단어를 읽어서 리스트로 변환합니다.\n",
    "    with open(positive_file, 'r', encoding='utf-8') as f:\n",
    "        positive_words = f.read().splitlines()\n",
    "\n",
    "    with open(negative_file, 'r', encoding='utf-8') as f:\n",
    "        negative_words = f.read().splitlines()\n",
    "\n",
    "    # 긍정적인 문장과 부정적인 문장을 학습 데이터로 생성합니다.\n",
    "    positive_sentences = [\" \".join(positive_words)] * len(positive_words)\n",
    "    negative_sentences = [\" \".join(negative_words)] * len(negative_words)\n",
    "\n",
    "    # 학습 데이터와 레이블을 생성합니다.\n",
    "    X = positive_sentences + negative_sentences\n",
    "    y = [1] * len(positive_sentences) + [0] * len(negative_sentences)\n",
    "\n",
    "    # CountVectorizer를 사용하여 단어의 빈도수를 측정합니다.\n",
    "    vectorizer = CountVectorizer(token_pattern=r\"\\b\\w+\\b\")\n",
    "    X = vectorizer.fit_transform(X)\n",
    "\n",
    "    # LogisticRegression을 사용하여 모델을 학습합니다.\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return vectorizer, clf\n",
    "\n",
    "def predict_sentiment_analysis(text, vectorizer, clf):\n",
    "    # 입력된 텍스트를 벡터화합니다.\n",
    "    X = vectorizer.transform([text])\n",
    "\n",
    "    # 모델을 사용하여 감성을 예측합니다.\n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    # 예측 결과에 따라 출력 메시지와 카운트 정보를 선택합니다.\n",
    "    if y_pred[0] == 1:\n",
    "        result = {\"sentiment\": \"긍정적인 단어\", \"positive_count\": 1, \"negative_count\": 0}\n",
    "    else:\n",
    "        result = {\"sentiment\": \"부정적인 단어\", \"positive_count\": 0, \"negative_count\": 1}\n",
    "\n",
    "    # 입력된 텍스트에 포함된 긍정 단어와 부정 단어를 카운트합니다.\n",
    "    for word in text.split():\n",
    "        if word in positive_words:\n",
    "            result[\"positive_count\"] += 1\n",
    "        elif word in negative_words:\n",
    "            result[\"negative_count\"] += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7bc886d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# clf 모델 객체를 생성하고 학습하는 코드\n",
    "\n",
    "# 모델을 파일로 저장\n",
    "with open('clf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "322cf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clf_model.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9ce90d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석할 문자열을 입력하세요: bad\n",
      "부정적인 문장입니다.\n"
     ]
    }
   ],
   "source": [
    "new_text = input(\"분석할 문자열을 입력하세요: \")\n",
    "result = predict_sentiment(new_text, vectorizer, clf)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3f94cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dc3ca368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from openpyxl import Workbook\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5dbf9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = Workbook(write_only=True)\n",
    "ws = wb.create_sheet()\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.get(\"https://www.youtube.com/watch?v=WGLhXmUpP34\")\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, 800)\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3d25ab08",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_css_selector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m buttons \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements_by_css_selector\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#more-replies > a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m button \u001b[38;5;129;01min\u001b[39;00m buttons:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_css_selector'"
     ]
    }
   ],
   "source": [
    "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_css_selector(\"#dismiss-button > a\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "buttons = driver.find_elements_by_css_selector(\"#more-replies > a\")\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "for button in buttons:\n",
    "    button.send_keys(Keys.ENTER)\n",
    "    time.sleep(1.5)\n",
    "    button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cf1a8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_source = driver.page_source\n",
    "soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "id_list = soup.select(\"div#header-author > h3 > #author-text > span\")\n",
    "comment_list = soup.select(\"yt-formatted-string#content-text\")\n",
    "\n",
    "id_final = []\n",
    "comment_final = []\n",
    "\n",
    "for i in range(len(comment_list)):\n",
    "    temp_id = id_list[i].text\n",
    "    temp_id = temp_id.replace('\\n', '')\n",
    "    temp_id = temp_id.replace('\\t', '')\n",
    "    temp_id = temp_id.replace('    ', '')\n",
    "    id_final.append(temp_id) # 댓글 작성자\n",
    "\n",
    "    temp_comment = comment_list[i].text\n",
    "    temp_comment = temp_comment.replace('\\n', '')\n",
    "    temp_comment = temp_comment.replace('\\t', '')\n",
    "    temp_comment = temp_comment.replace('    ', '')\n",
    "    comment_final.append(temp_comment) # 댓글 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "90018f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = {\"아이디\" : id_final , \"댓글 내용\" : comment_final}\n",
    "youtube_pd = pd.DataFrame(pd_data)\n",
    "\n",
    "youtube_pd.to_excel('result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5d1261d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanum Pen Script C:\\Windows\\Fonts\\NanumPen.ttf\n"
     ]
    }
   ],
   "source": [
    "#워드클라우드\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "for f in fm.fontManager.ttflist:\n",
    "    if 'Nanum' in f.name:\n",
    "        print(f.name, f.fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b8156d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = 'C:/indows/Fonts/NanumPen.ttf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ccf8dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"ㅋㅋ\")\n",
    "stopwords.add(\"ㅎㅎ\")\n",
    "stopwords.add(\"너무\")\n",
    "stopwords.add(\"진짜\")\n",
    "stopwords.add(\"와\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1703c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub(r'\\b[ㅋㅎ]{3,}\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9a234721",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'min_word_frequency'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [215]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(comment_list)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 워드 클라우드 생성\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_word_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 워드 클라우드 출력\u001b[39;00m\n\u001b[0;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m), facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'min_word_frequency'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV 파일을 pandas 데이터프레임으로 읽어옴\n",
    "df = pd.read_csv('C:/Users/tiock/Desktop/수업자료/인공지능/text/result.csv',encoding='cp949')\n",
    "\n",
    "# 댓글 내용이 저장된 컬럼 선택\n",
    "comment_col = '댓글 내용'\n",
    "comment_list = df[comment_col].tolist()\n",
    "comment_list = [str(x) for x in comment_list]\n",
    "# 모든 댓글 내용을 하나의 문자열로 결합\n",
    "text = ' '.join(comment_list)\n",
    "\n",
    "# 워드 클라우드 생성\n",
    "wordcloud = WordCloud(font_path=font_path,width=800, height=800,\n",
    "                      background_color='white',stopwords=stopwords, min_word_frequency=10).generate(text)\n",
    "\n",
    "# 워드 클라우드 출력\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# 파일로 저장\n",
    "plt.savefig('wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0dedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 사이즈 흑백 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe369a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiock\\AppData\\Local\\Temp\\ipykernel_67000\\136393486.py:16: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  img = img.resize(desired_size, resample=Image.BILINEAR)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "input_dir = 'C:/Users/tiock/Desktop/수업자료/인공지능/text/'\n",
    "output_dir = 'C:/Users/tiock/Desktop/수업자료/인공지능/'\n",
    "desired_size = (512, 512)  # Change to the desired size\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "        # Load image\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        with Image.open(input_path) as img:\n",
    "            # Convert to grayscale\n",
    "            img = img.convert('L')\n",
    "            # Resize image\n",
    "            img = img.resize(desired_size, resample=Image.BILINEAR)\n",
    "            # Save resized image\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            img.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a85ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item no.: 1 --> Item name = Polar bears\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "\n",
      "\n",
      "Unfortunately all 20 could not be downloaded because some images were not downloadable. 0 is all we got for this search filter!\n",
      "\n",
      "Errors: 0\n",
      "\n",
      "\n",
      "Item no.: 2 --> Item name = baloons\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "\n",
      "\n",
      "Unfortunately all 20 could not be downloaded because some images were not downloadable. 0 is all we got for this search filter!\n",
      "\n",
      "Errors: 0\n",
      "\n",
      "\n",
      "Item no.: 3 --> Item name = Beaches\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "\n",
      "\n",
      "Unfortunately all 20 could not be downloaded because some images were not downloadable. 0 is all we got for this search filter!\n",
      "\n",
      "Errors: 0\n",
      "\n",
      "({'Polar bears': [], 'baloons': [], 'Beaches': []}, 0)\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download  \n",
    "\n",
    "response = google_images_download.googleimagesdownload()   \n",
    "\n",
    "arguments = {\"keywords\":\"Polar bears,baloons,Beaches\",\"limit\":20,\"print_urls\":True,\"format\": \"jpg\"}   \n",
    "paths = response.download(arguments)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4627e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
